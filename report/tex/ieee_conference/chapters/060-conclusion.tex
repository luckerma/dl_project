\section{Conclusion and Lessons Learned}

\subsection{Key Takeaways}

In this project, several strategies were explored to classify plant seedlings into 12 different species, with the overall goal of achieving robust performance as measured by the mean (micro-averaged) F1-score. Data augmentation played a central role in preventing overfitting and improving model performance. Techniques such as random rotations, flips and color jittering effectively increased the diversity of the training samples, thereby improving the robustness of the learned feature representations. Meanwhile, the choice of an appropriate model architecture proved critical. A custom CNN designed and trained from scratch achieved competitive results (F1-score of 0.92695), demonstrating the potential for custom solutions even with relatively modest dataset sizes. However, the use of pre-trained networks, such as ResNet-18 and vit-base-patch16-224, demonstrated how transfer learning can deliver superior results (F1-scores of 0.96095 and 0.96725) by building on rich feature embeddings learned from large-scale datasets. Proper validation underpinned these successes, with a stratified split ensuring balanced class distributions in both the training and validation sets. This practice not only prevented the model from overfitting to majority classes, but also allowed careful monitoring of loss and accuracy metrics to guide training decisions and allowed early stopping to load the best model state before overfitting occurred.

\subsection{Challenges Encountered}

Despite the encouraging results, several challenges remained throughout the process. The class imbalance present in the dataset contributed to occasional misclassifications, underscoring the need for robust strategies to deal with skewed data. In addition, certain class pairs, such as "Loose Silky-bent" and ``Black-grass'', exhibited high visual similarity, leading to consistent confusion for both the custom and pre-trained models. Overfitting remained a significant risk due to the limited dataset size, necessitating the use of multiple regularization methods including weight decay, dropout layers and data augmentation to ensure generalization. Computational constraints also played a role in decisions regarding batch size, image resolution and the complexity of architectures that could be feasibly trained within the available resources (e.g., freezing layers in the ViT model to reduce trainable parameters). Finally, the lack of labeled test data made it difficult to comprehensively evaluate the models, necessitating the use of the validation set as a proxy for performance on unseen data.

\subsection{Future Work}

Going forward, there are several ways to refine and extend the current results. Adding more models to the ensemble or training the ensemble on the validation set to find the optimal weights for each model. Exploring deeper pre-trained networks such as ResNet-50, DenseNet, or EfficientNet could improve performance, although careful management of overfitting will be important. Collecting additional labeled data or generating synthetic samples using generative adversarial networks (GANs) could help address the class imbalance and improve the ability of the model to generalize to underrepresented classes.
