\section{Conclusion and Lessons Learned}

This section summarizes the key findings and lessons learned from the project. It discusses key takeaways, challenges encountered, a comparison to other challenge submissions and papers and potential future work to improve the results.

\subsection{Key Takeaways}

In this project, several strategies were explored to classify plant seedlings into \textit{12} different species, with the overall goal of achieving robust performance as measured by the mean~(micro-averaged) F1-score. Data augmentation played a central role in preventing overfitting and improving model performance. Techniques such as random rotations, flips and color jittering effectively increased the diversity of the training samples, thereby improving the robustness of the learned feature representations. Meanwhile, the choice of an appropriate model architecture proved critical. A custom CNN designed and trained from scratch achieved competitive results (test F1-score of \textbf{0.92695}), demonstrating the potential for custom solutions even with relatively modest dataset sizes. However, the use of pre-trained networks, such as \texttt{ResNet18} and \texttt{vit-base-patch16-224}, demonstrated how transfer learning can deliver superior results (test F1-scores of \textbf{0.96095} and \textbf{0.96725}) by building on rich feature embeddings learned from large-scale datasets. Proper validation underlined these successes, with a stratified split ensuring balanced class distributions in both the training and validation sets. This practice not only prevented the model from overfitting to majority classes, but also allowed careful monitoring of loss and accuracy metrics to guide training decisions and allowed early stopping to load the best model state before overfitting occurred.

\subsection{Challenges Encountered}

Despite the encouraging results, several challenges remained throughout the process. The class imbalance present in the dataset underscores the need for robust strategies to deal with skewed data, such as stratified data splits. In addition, certain class pairs, such as ``Loose Silky-bent'' and ``Black-grass'', exhibited high visual similarity, leading to consistent confusion for all the custom and pre-trained models. Overfitting remained a significant risk due to the limited dataset size, necessitating the use of multiple regularization methods including weight decay, dropout layers and data augmentation to ensure generalization. Computational constraints also played a role in decisions regarding batch size, image resolution and the capacity of architectures that could be feasibly trained within the available resources (e.g., freezing layers in the ViT model to reduce trainable parameters). Finally, the unavailability of labels for the test set made it difficult to comprehensively evaluate the models, necessitating the use of the validation set as a proxy for performance on unseen data.

\subsection{Kaggle Challenge Comparison}

The Kaggle challenge provided a valuable benchmark to contextualize the performance described. Although the official competition ended in March, 2018, making further submissions and rankings comparisons impossible, analysis of the historical results still provides valuable insights.

The ensemble model achieved a final F1-score of \textbf{0.97103} on the test set, placing it approximately at rank \textit{327} out of \textit{833} public participants. Notably, the competition was highly competitive, with the top two submissions achieving perfect scores of 1.0. Unfortunately, the detailed methodologies and codebases of these top-ranked submissions are not publicly available, limiting direct comparison.

However, several published solutions provide useful reference points. The best publicly documented model used the InceptionResNetV2 architecture~\cite{DBLP:journals/corr/SzegedyIV16} and achieved an score of 0.98740, closely followed by another highly successful solution based on EfficientNetB0~\cite{DBLP:journals/corr/abs-1905-11946}. In addition,~\cite{8650178} reported impressive results~(99.69\% accuracy) by combining AlexNet~\cite{NIPS2012_c399862d} with transfer learning, along with extensive preprocessing steps such as color space conversion and image enhancement. Another study~\cite{hassan2021plant} demonstrated strong performance~(97.54\%) with the VGG19 network~\cite{simonyan2015deepconvolutionalnetworkslargescale}, outperforming ResNet models in their specific experiments.

This paper is closely aligned with these findings, highlighting the effectiveness of transfer learning for image classification tasks, especially when faced with dataset limitations. However, the uniqueness of this work lies in the explicit investigation of a spectrum of techniques ranging from custom architectures to advanced transfer models such as ViTs, all trained locally and the custom CNN even trained from scratch, while achieving competitive results.

\subsection{Future Work}

Going forward, there are several ways to refine and extend the current results. Adding more models to the ensemble or training the ensemble on the validation set data to find the optimal weights for each model. Exploring deeper pre-trained networks such as ResNet-50, DenseNet, or EfficientNet could improve performance, although careful management of overfitting will be important. Collecting additional labeled data or generating synthetic samples using generative adversarial networks~(GANs)~\cite{goodfellow2014generativeadversarialnetworks} could help address the class imbalance and improve the ability of the model to generalize to underrepresented classes. Finally, and probably the best next step, is image segmentation to remove the background ``noise'' and focus on the plant seedling itself. This could help the models learn more relevant features and improve classification accuracy.