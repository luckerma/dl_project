<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-28">

<title>Plant Seedlings Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-4c3f21f622398162bebe0d2c2d806a7a.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Plant Seedlings Classification">
<meta name="citation_abstract" content="This paper explores a Plant Seedlings Classification dataset consisting of multiple images representing *12* different plant species. Several deep learning approaches are presented, including a custom CNN trained from scratch, a pre-trained CNN (ResNet-18) and a pre-trained vision transformer (vit-base-patch16-224) tuned for this classification task. To address the challenges of data scarcity and class imbalance, extensive data augmentation techniques such as random rotations, flips and color jittering are employed. Results show that transfer learning with ResNet-18 outperforms the custom model, achieving a mean F1-score (micro-averaged) of *0.96095* on the test set. The custom CNN, while slightly less accurate, still achieves a competitive F1-score of *0.92695*, demonstrating that even smaller locally trained architectures can be viable if carefully designed and thoroughly regularized. While the vision transformer model achieves a high F1-score of *0.96725*, an ensemble combining the predictions of all three models achieves the highest F1-score of *0.97103*. Finally, potential solutions are outlined, including deeper architectures, synthetic augmentation and interpretability measures, to further improve seedling classification performance.
">
<meta name="citation_author" content="Luca Uckermann">
<meta name="citation_author" content="Nikethan Nimalakumaran">
<meta name="citation_author" content="Jonas Möller">
<meta name="citation_publication_date" content="2025-03-28">
<meta name="citation_cover_date" content="2025-03-28">
<meta name="citation_year" content="2025">
<meta name="citation_online_date" content="2025-03-28">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=ImageNet: A large-scale hierarchical image database;,citation_author=Jia Deng;,citation_author=Wei Dong;,citation_author=Richard Socher;,citation_author=Li-Jia Li;,citation_author=Kai Li;,citation_author=Li Fei-Fei;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_doi=10.1109/CVPR.2009.5206848;,citation_conference_title=2009 IEEE conference on computer vision and pattern recognition;">
<meta name="citation_reference" content="citation_title=A public image database for benchmark of plant seedling classification algorithms;,citation_author=Thomas Mosgaard Giselsson;,citation_author=Rasmus Nyholm Jørgensen;,citation_author=Peter Kryger Jensen;,citation_author=Mads Dyrmann;,citation_author=Henrik Skov Midtiby;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=http://arxiv.org/abs/1711.05458;,citation_volume=abs/1711.05458;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=An image is worth 16x16 words: Transformers for image recognition at scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2010.11929;,citation_volume=abs/2010.11929;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Deep residual learning for image recognition;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=http://arxiv.org/abs/1512.03385;,citation_volume=abs/1512.03385;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=PyTorch library for CAM methods;,citation_author=Jacob Gildenblat;,citation_author=undefined contributors;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_publisher=https://github.com/jacobgil/pytorch-grad-cam; GitHub;">
<meta name="citation_reference" content="citation_title=Adam: A method for stochastic optimization;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1412.6980;">
<meta name="citation_reference" content="citation_title=Machine learning in agriculture domain: A state-of-art survey;,citation_abstract=Food is considered as a basic need of human being which can be satisfied through farming. Agriculture not only fulfills humans’ basic needs, but also considered as source of employment worldwide. Agriculture is considered as a backbone of economy and source of employment in the developing countries like India. Agriculture contributes 15.4;,citation_author=Vishal Meshram;,citation_author=Kailas Patil;,citation_author=Vidula Meshram;,citation_author=Dinesh Hanchate;,citation_author=S. D. Ramkteke;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S2667318521000106;,citation_doi=https://doi.org/10.1016/j.ailsci.2021.100010;,citation_issn=2667-3185;,citation_volume=1;,citation_journal_title=Artificial Intelligence in the Life Sciences;">
<meta name="citation_reference" content="citation_title=Plant seedlings classification;,citation_author=undefined inversion;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_publisher=https://kaggle.com/competitions/plant-seedlings-classification;">
<meta name="citation_reference" content="citation_title=Plant seedlings classification;,citation_author=undefined inversion;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_publisher=www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation;">
<meta name="citation_reference" content="citation_title=PyTorch Image Models;,citation_author=Ross Wightman;,citation_fulltext_html_url=https://github.com/huggingface/pytorch-image-models;,citation_doi=10.5281/zenodo.4414861;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Plant Seedlings Classification</h1>
            <p class="subtitle lead">UGE: M2 SIA - DL Project Report</p>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Luca Uckermann <a href="mailto:luca_simon.uckermann@smail.th-koeln.de" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> <a href="https://orcid.org/0009-0005-2957-6331" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Applied Sciences (TH Köln)
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Nikethan Nimalakumaran </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Université Gustave Eiffel
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Jonas Möller </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Bielefeld University
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">March 28, 2025</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This paper explores a Plant Seedlings Classification dataset consisting of multiple images representing <em>12</em> different plant species. Several deep learning approaches are presented, including a custom CNN trained from scratch, a pre-trained CNN (ResNet-18) and a pre-trained vision transformer (vit-base-patch16-224) tuned for this classification task. To address the challenges of data scarcity and class imbalance, extensive data augmentation techniques such as random rotations, flips and color jittering are employed. Results show that transfer learning with ResNet-18 outperforms the custom model, achieving a mean F1-score (micro-averaged) of <em>0.96095</em> on the test set. The custom CNN, while slightly less accurate, still achieves a competitive F1-score of <em>0.92695</em>, demonstrating that even smaller locally trained architectures can be viable if carefully designed and thoroughly regularized. While the vision transformer model achieves a high F1-score of <em>0.96725</em>, an ensemble combining the predictions of all three models achieves the highest F1-score of <em>0.97103</em>. Finally, potential solutions are outlined, including deeper architectures, synthetic augmentation and interpretability measures, to further improve seedling classification performance.</p>
      </div>
    </div>


    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-problem-understanding" id="toc-introduction-problem-understanding" class="nav-link active" data-scroll-target="#introduction-problem-understanding"><span class="header-section-number">1</span> Introduction &amp; Problem Understanding</a>
  <ul class="collapse">
  <li><a href="#context-and-background" id="toc-context-and-background" class="nav-link" data-scroll-target="#context-and-background"><span class="header-section-number">1.1</span> Context and Background</a></li>
  <li><a href="#problem-definition-and-objectives" id="toc-problem-definition-and-objectives" class="nav-link" data-scroll-target="#problem-definition-and-objectives"><span class="header-section-number">1.2</span> Problem Definition and Objectives</a></li>
  <li><a href="#dataset-overview" id="toc-dataset-overview" class="nav-link" data-scroll-target="#dataset-overview"><span class="header-section-number">1.3</span> Dataset Overview</a></li>
  <li><a href="#key-challenges" id="toc-key-challenges" class="nav-link" data-scroll-target="#key-challenges"><span class="header-section-number">1.4</span> Key Challenges</a></li>
  </ul></li>
  <li><a href="#model-architecture-design" id="toc-model-architecture-design" class="nav-link" data-scroll-target="#model-architecture-design"><span class="header-section-number">2</span> Model Architecture Design</a>
  <ul class="collapse">
  <li><a href="#guessing-baseline" id="toc-guessing-baseline" class="nav-link" data-scroll-target="#guessing-baseline"><span class="header-section-number">2.1</span> Guessing Baseline</a></li>
  <li><a href="#custom-cnn" id="toc-custom-cnn" class="nav-link" data-scroll-target="#custom-cnn"><span class="header-section-number">2.2</span> Custom CNN</a></li>
  <li><a href="#pre-trained-cnn" id="toc-pre-trained-cnn" class="nav-link" data-scroll-target="#pre-trained-cnn"><span class="header-section-number">2.3</span> Pre-trained CNN</a></li>
  <li><a href="#pre-trained-vit" id="toc-pre-trained-vit" class="nav-link" data-scroll-target="#pre-trained-vit"><span class="header-section-number">2.4</span> Pre-trained ViT</a></li>
  <li><a href="#ensemble" id="toc-ensemble" class="nav-link" data-scroll-target="#ensemble"><span class="header-section-number">2.5</span> Ensemble</a></li>
  </ul></li>
  <li><a href="#training-optimization-strategies" id="toc-training-optimization-strategies" class="nav-link" data-scroll-target="#training-optimization-strategies"><span class="header-section-number">3</span> Training Optimization Strategies</a>
  <ul class="collapse">
  <li><a href="#sec-training-optimization" id="toc-sec-training-optimization" class="nav-link" data-scroll-target="#sec-training-optimization"><span class="header-section-number">3.1</span> Training Algorithms &amp; Optimizers</a></li>
  <li><a href="#learning-rate-schedules" id="toc-learning-rate-schedules" class="nav-link" data-scroll-target="#learning-rate-schedules"><span class="header-section-number">3.2</span> Learning Rate Schedules</a></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques"><span class="header-section-number">3.3</span> Regularization Techniques</a></li>
  </ul></li>
  <li><a href="#model-evaluation-validation" id="toc-model-evaluation-validation" class="nav-link" data-scroll-target="#model-evaluation-validation"><span class="header-section-number">4</span> Model Evaluation &amp; Validation</a>
  <ul class="collapse">
  <li><a href="#validation-framework" id="toc-validation-framework" class="nav-link" data-scroll-target="#validation-framework"><span class="header-section-number">4.1</span> Validation Framework</a></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="header-section-number">4.2</span> Performance Metrics</a></li>
  </ul></li>
  <li><a href="#results-analysis" id="toc-results-analysis" class="nav-link" data-scroll-target="#results-analysis"><span class="header-section-number">5</span> Results &amp; Analysis</a>
  <ul class="collapse">
  <li><a href="#quantitative-results" id="toc-quantitative-results" class="nav-link" data-scroll-target="#quantitative-results"><span class="header-section-number">5.1</span> Quantitative Results</a></li>
  <li><a href="#qualitative-results" id="toc-qualitative-results" class="nav-link" data-scroll-target="#qualitative-results"><span class="header-section-number">5.2</span> Qualitative Results</a></li>
  <li><a href="#comparative-analysis" id="toc-comparative-analysis" class="nav-link" data-scroll-target="#comparative-analysis"><span class="header-section-number">5.3</span> Comparative Analysis</a></li>
  <li><a href="#interpretability-measures" id="toc-interpretability-measures" class="nav-link" data-scroll-target="#interpretability-measures"><span class="header-section-number">5.4</span> Interpretability Measures</a></li>
  </ul></li>
  <li><a href="#conclusion-lessons-learned" id="toc-conclusion-lessons-learned" class="nav-link" data-scroll-target="#conclusion-lessons-learned"><span class="header-section-number">6</span> Conclusion &amp; Lessons Learned</a>
  <ul class="collapse">
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">6.1</span> Key Takeaways</a></li>
  <li><a href="#challenges-encountered" id="toc-challenges-encountered" class="nav-link" data-scroll-target="#challenges-encountered"><span class="header-section-number">6.2</span> Challenges Encountered</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work"><span class="header-section-number">6.3</span> Future Work</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction-problem-understanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction &amp; Problem Understanding</h1>
<section id="context-and-background" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="context-and-background"><span class="header-section-number">1.1</span> Context and Background</h2>
<p>The “Plant Seedlings Classification” challenge, hosted on Kaggle <span class="citation" data-cites="plant-seedlings-classification">(<a href="#ref-plant-seedlings-classification" role="doc-biblioref">inversion 2017a</a>)</span>, presents a real-world problem central to modern agriculture: accurately identifying the species of young seedlings from digital images.</p>
<p>The dataset described in the paper “A Public Image Database for Benchmark of Plant Seedling Classification Algorithms” <span class="citation" data-cites="DBLP:journals/corr/abs-1711-05458">(<a href="#ref-DBLP:journals/corr/abs-1711-05458" role="doc-biblioref">Giselsson et al. 2017</a>)</span> contains images of approximately <em>960</em> unique plants, representing <em>12</em> different species. Each image captures a seedling at different growth stages and under different conditions, reflecting the complexities found in real-world agricultural environments. These conditions include differences in lighting, background soil patterns and subtle phenotypic variations that can blur the lines between certain species. The evaluation metric of the competition is a mean (micro-averaged) F1-score, which encourages balanced performance across classes <span class="citation" data-cites="plant-seedlings-classification-evaluation">(<a href="#ref-plant-seedlings-classification-evaluation" role="doc-biblioref">inversion 2017b</a>)</span>:</p>
<p><span id="eq-precision"><span class="math display">\[
\text{Precision}_{\text{micro}} = \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FP_k}
\tag{1}\]</span></span></p>
<p><span id="eq-recall"><span class="math display">\[
\text{Recall}_{\text{micro}} = \frac{\sum_{k \in C} TP*k}{\sum*{k \in C} TP_k + FN_k}
\tag{2}\]</span></span></p>
<p><span id="eq-fscore"><span class="math display">\[
F1*{\text{micro}} = \frac{2 \cdot \text{Precision}*{\text{micro}} \cdot \text{Recall}_{\text{micro}}}{\text{Precision}_{\text{micro}} + \text{Recall}\_{\text{micro}}}
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(TP_i\)</span>, <span class="math inline">\(FP_i\)</span> and <span class="math inline">\(FN_i\)</span> are the true positive, false positive and false negative counts for class <span class="math inline">\(i\)</span>, respectively and <span class="math inline">\(C\)</span> is the set of all classes. The mean F1-score (<a href="#eq-fscore" class="quarto-xref">Equation&nbsp;3</a>) is a balanced measure that considers both precision (<a href="#eq-precision" class="quarto-xref">Equation&nbsp;1</a>) and recall (<a href="#eq-recall" class="quarto-xref">Equation&nbsp;2</a>) across all classes, making it a suitable evaluation metric for multi-class classification tasks.</p>
</section>
<section id="problem-definition-and-objectives" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="problem-definition-and-objectives"><span class="header-section-number">1.2</span> Problem Definition and Objectives</h2>
<p>The core objective of the challenge is to build an automated classification model that can take a seedling image as input and accurately predict its species. The following points summarize the task:</p>
<ul>
<li><strong>Input:</strong> Set of 794 images of plant seedlings.</li>
<li><strong>Output:</strong> Classification label for each image, indicating the species of each plant seedling.</li>
<li><strong>Goal:</strong> High classification performance as measured by the <a href="#eq-fscore" class="quarto-xref">Equation&nbsp;3</a>.</li>
</ul>
</section>
<section id="dataset-overview" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="dataset-overview"><span class="header-section-number">1.3</span> Dataset Overview</h2>
<p>For a better understanding of the dataset, a brief overview of the class distribution and sample images is provided below:</p>
<div id="fig-class-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-class-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/class_distribution.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Class Distribution"><img src="resources/class_distribution.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-class-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Class Distribution
</figcaption>
</figure>
</div>
<p><a href="#fig-class-distribution" class="quarto-xref">Figure&nbsp;1</a> shows the distribution of classes in the train dataset, with each bar representing the number of images per class. The dataset is imbalanced, with some classes having significantly fewer samples than others. This imbalance can pose a challenge for model training, as the model may struggle to learn the features of underrepresented classes effectively. The most common classes are “Loose Silky-bent” (654) and “Common Chickweed” (611), while the least common classes are “Common wheat” and “Maize” (both 221).</p>
<div id="fig-sample-images" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sample-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/sample_images.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Sample Images"><img src="resources/sample_images.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sample-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Sample Images
</figcaption>
</figure>
</div>
<p><a href="#fig-sample-images" class="quarto-xref">Figure&nbsp;2</a> shows a sample image for each class in the dataset, illustrating the visual diversity in different species. The images vary in background, lighting and growth stage, highlighting the challenges of visual similarity across species.</p>
</section>
<section id="key-challenges" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="key-challenges"><span class="header-section-number">1.4</span> Key Challenges</h2>
<p>Developing robust classification models for this task is not trivial. There are several challenges:</p>
<ol type="1">
<li><strong>Visual Similarity Among Species:</strong> Certain seedlings can look strikingly similar, making it difficult for both humans and machines to distinguish between them.</li>
<li><strong>Intra-Class Variability:</strong> Even within a single species, seedlings can vary significantly in appearance due to differences in growth stage, lighting and background. This variability challenges models to learn consistent features that generalize well.</li>
<li><strong>Data Limitations:</strong> With approximately 960 unique plants, the dataset could be considered modest for training deep learning models from scratch. While data augmentation can help to some extent, the relatively small dataset may still limit the complexity of models that can be effectively trained without overfitting.</li>
<li><strong>Model Architecture Complexity:</strong> Choosing the right model architecture, whether a custom CNN trained from scratch or a pre-trained deep CNN or Vision Transformer (ViT), to learn complex visual features. Deeper models can capture more nuanced differences, but they can also be harder to train and require careful regularization to prevent overfitting.</li>
</ol>
<p>By clearly understanding these challenges and the broader context, model architectures can be proposed that address these difficulties. The following chapters discuss the strategies for model design, training optimization and thorough evaluation, ultimately leading to the approach that best addresses the core challenge of differentiating between plant seedling species.</p>
</section>
</section>
<section id="model-architecture-design" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Model Architecture Design</h1>
<p>To achieve deterministic results and reproducibility, the random seed <em>42</em> is set at the beginning of each script. This ensures that the same random initialization is used for each run, leading to consistent results across different experiments:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>seed(RANDOM_SEED)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(RANDOM_SEED)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(RANDOM_SEED)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed_all(RANDOM_SEED)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The random seed is set for the Python, NumPy and PyTorch random number generator. Additionally, the CuDNN backend is set to deterministic mode to ensure that the results are reproducible on the GPU.</p>
<section id="guessing-baseline" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="guessing-baseline"><span class="header-section-number">2.1</span> Guessing Baseline</h2>
<p>As a starting point and to get familiar with the dataset and the Kaggle competition, a simple guessing baseline is implemented. The baseline assigns the most frequent class label to all test samples. This approach provides a lower bound on model performance and serves as a reference point for evaluating the effectiveness of more sophisticated models. The head of the submission file (<code>submission-0.14105_Loose-Silky-bent.csv</code>) is shown below:</p>
<pre class="csv"><code>file,species
1b490196c.png,Loose Silky-bent
85431c075.png,Loose Silky-bent
506347cfe.png,Loose Silky-bent
7f46a71db.png,Loose Silky-bent
668c1007c.png,Loose Silky-bent
...</code></pre>
<p>In this case, all <em>794</em> test samples are assigned the class label “Loose Silky-bent”, which is the most frequent class in the training dataset. The F1-score of this baseline is <em>0.14105</em>.</p>
</section>
<section id="custom-cnn" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="custom-cnn"><span class="header-section-number">2.2</span> Custom CNN</h2>
<p>The custom CNN architecture is designed to capture features relevant to seedling classification, while being lightweight enough to be effectively trained locally on the given dataset. The model consists of a series of convolutional and pooling layers followed by fully connected layers to learn hierarchical features and make class predictions.</p>
<div id="fig-custom-cnn-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-custom-cnn-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Custom CNN Architecture"><img src="resources/custom_cnn/architecture.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-custom-cnn-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Custom CNN Architecture
</figcaption>
</figure>
</div>
<p>As shown in <a href="#fig-custom-cnn-architecture" class="quarto-xref">Figure&nbsp;3</a>, the network begins with a series of convolutional layers, with the number of filters gradually increasing from 16 to 256. These convolutional layers, each followed by a Rectified Linear Unit (ReLU) activation, extract spatial features such as edges, textures and patterns from the images. To reduce spatial dimensions and computational complexity, max-pooling layers are applied after each convolutional block to focus on the most salient features.</p>
<p>After the convolutional and pooling stages, the feature maps are flattened into a 1D vector that serves as the input to the fully connected layers. The first fully connected layer has 128 units and captures high-level abstract features, while the final fully connected layer maps these features to the 12 target classes, producing the class probabilities.</p>
<pre class="plaintext"><code>================================================================
Total params: 1,999,916
Trainable params: 1,999,916
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 26.70
Params size (MB): 7.63
Estimated Total Size (MB): 34.91
----------------------------------------------------------------</code></pre>
<p>The final custom CNN model has approximately <em>2 million</em> parameters, making it lightweight and computationally efficient. The model architecture is designed to capture relevant features for seedling classification while being suitable for training on a moderate-sized dataset.</p>
</section>
<section id="pre-trained-cnn" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="pre-trained-cnn"><span class="header-section-number">2.3</span> Pre-trained CNN</h2>
<p>As an alternative to training a custom CNN from scratch, a pre-trained CNN can be used to leverage learned features from a large dataset. The pre-trained model ResNet-18 <span class="citation" data-cites="DBLP:journals/corr/HeZRS15">(<a href="#ref-DBLP:journals/corr/HeZRS15" role="doc-biblioref">He et al. 2015</a>)</span> is used as a feature extractor, where the final classification layer is replaced with a new fully connected layer to predict the 12 plant seedling classes:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Linear</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet18(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    weights<span class="op">=</span>models.ResNet18_Weights.DEFAULT</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> Linear(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    in_features<span class="op">=</span>model.fc.in_features,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    out_features<span class="op">=</span><span class="bu">len</span>(dataset.classes),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The ResNet-18 model is pre-trained on the ImageNet dataset <span class="citation" data-cites="5206848ImageNet">(<a href="#ref-5206848ImageNet" role="doc-biblioref">Deng et al. 2009</a>)</span> and has shown strong performance on a variety of computer vision tasks. By using a pre-trained model, the network can leverage the learned features from ImageNet to improve performance on the plant seedlings dataset. The final classification layer is replaced to adapt the model to the specific classification task.</p>
<pre class="plaintext"><code>================================================================
Total params: 11,182,668
Trainable params: 11,182,668
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 62.79
Params size (MB): 42.66
Estimated Total Size (MB): 106.02
----------------------------------------------------------------</code></pre>
<p>This pre-trained CNN model has approximately <em>11 million</em> parameters, making it more complex than the custom CNN. However, the pre-trained weights allow the model to learn more robust features and hopefully achieve better performance on the plant seedlings dataset.</p>
</section>
<section id="pre-trained-vit" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="pre-trained-vit"><span class="header-section-number">2.4</span> Pre-trained ViT</h2>
<p>Another approach is to use a ViT <span class="citation" data-cites="DBLP:journals/corr/abs-2010-11929">(<a href="#ref-DBLP:journals/corr/abs-2010-11929" role="doc-biblioref">Dosovitskiy et al. 2020</a>)</span> as the backbone architecture. The ViT model is pre-trained on a large-scale dataset and then fine-tuned on the plant seedlings dataset. The final classification head is replaced with a new linear layer to predict the 12 plant seedling classes:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vit_base_patch16_224"</span>,</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span>num_classes</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>model.head <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    model.head.in_features,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    num_classes</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Instead of fine-tuning the entire model, the pre-trained weights of the <code>vit_base_patch16_224</code> model <span class="citation" data-cites="Wightman_PyTorch_Image_Models">(<a href="#ref-Wightman_PyTorch_Image_Models" role="doc-biblioref">Wightman, n.d.</a>)</span> are frozen, and only the classification head is trained on the plant seedlings dataset. This approach leverages the powerful feature extraction capabilities of the pre-trained model while adapting the final layer to the specific classification task. Furthermore the computational cost is reduced compared to training the entire model from scratch or fine-tuning all layers:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.head.parameters():</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre class="plaintext"><code>================================================================
Total params: 85,655,820
Trainable params: 9,228
Non-trainable params: 85,646,592
----------------------------------------------------------------
Input size (MB): 0.57
Forward/backward pass size (MB): 479.03
Params size (MB): 326.75
Estimated Total Size (MB): 806.35
----------------------------------------------------------------</code></pre>
<p>The ViT model has approximately <em>85 million</em> parameters, but only <em>9,228</em> of them are trainable. This makes the model computationally efficient while still benefiting from the powerful feature extraction capabilities of the pre-trained ViT model.</p>
</section>
<section id="ensemble" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="ensemble"><span class="header-section-number">2.5</span> Ensemble</h2>
<p>A final ensemble is created by combining the predictions of all three models (custom CNN, pre-trained CNN, pre-trained ViT) using a simple weighted average. The weights are determined based on the performance of each model on the test set:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>model_custom_cnn.<span class="bu">eval</span>()</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>model_resnet.<span class="bu">eval</span>()</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model_vit.<span class="bu">eval</span>()</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>w_custom_cnn <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>w_resnet <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>w_vit <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> w_custom_cnn <span class="op">-</span> w_resnet</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, image_names <span class="kw">in</span> test_loader:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        probs_custom_cnn <span class="op">=</span> F.softmax(model_custom_cnn(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>        probs_resnet <span class="op">=</span> F.softmax(model_resnet(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>        probs_vit <span class="op">=</span> F.softmax(model_vit(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>        probs_ensemble <span class="op">=</span> w_custom_cnn <span class="op">*</span> probs_custom_cnn <span class="op">+</span> w_resnet <span class="op">*</span> probs_resnet <span class="op">+</span> w_vit <span class="op">*</span> probs_vit</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>        _, preds <span class="op">=</span> torch.<span class="bu">max</span>(probs_ensemble, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The ensemble combines the strengths of each individual model to improve overall performance and robustness. By averaging the predictions of multiple models, the ensemble can reduce the impact of individual model weaknesses and provide more reliable predictions. As the ViT model achieved the highest performance on the test set, it is assigned the highest weight in the ensemble (0.5), while the custom CNN and ResNet models are assigned equal weights (both 0.25).</p>
</section>
</section>
<section id="training-optimization-strategies" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Training Optimization Strategies</h1>
<section id="sec-training-optimization" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-training-optimization"><span class="header-section-number">3.1</span> Training Algorithms &amp; Optimizers</h2>
<p>All models were trained using the Adam optimizer <span class="citation" data-cites="kingma2017adammethodstochasticoptimization">(<a href="#ref-kingma2017adammethodstochasticoptimization" role="doc-biblioref">Kingma and Ba 2017</a>)</span> with a learning rate of <em>0.001</em>. The Adam optimizer is a popular choice for training deep neural networks due to its adaptive learning rate mechanism and momentum-based updates. A weight decay of <em>1e-4</em> was applied to regularize the model and prevent overfitting:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(),</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="learning-rate-schedules" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="learning-rate-schedules"><span class="header-section-number">3.2</span> Learning Rate Schedules</h2>
<p>To adjust the learning rate during training, a learning rate scheduler was used to reduce the learning rate by a factor of <em>0.5</em> if the validation loss did not improve for <em>2</em> epochs. This technique helps the model converge more effectively by gradually reducing the learning rate as it approaches a local minimum:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> ReduceLROnPlateau</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"min"</span>,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    factor<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="regularization-techniques" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="regularization-techniques"><span class="header-section-number">3.3</span> Regularization Techniques</h2>
<p>To prevent overfitting and improve generalization, several regularization techniques were applied during training:</p>
<ul>
<li><strong>Weight Decay:</strong> L2 regularization with a weight decay of <em>1e-4</em> was applied to the optimizer to penalize large weights. (see <a href="#sec-training-optimization" class="quarto-xref">Section&nbsp;3.1</a>)</li>
<li><strong>Dropout:</strong> A dropout layer with a dropout probability of <em>0.5</em> was added after the fully connected layer to regularize the model and prevent co-adaptation of neurons:</li>
</ul>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Dropout</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.droupout <span class="op">=</span> Dropout(p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li><strong>Data Augmentation:</strong> Various data augmentation techniques such as random rotations, flips and color jittering were applied to the training images to increase the diversity of the training set and improve the robustness of the model:</li>
</ul>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose(</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        transforms.Resize(</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>transform_resize</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        transforms.RandomResizedCrop(</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>transform_resize,</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>            scale<span class="op">=</span>(<span class="fl">0.8</span>, <span class="fl">1.0</span>),</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>            ratio<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">1.1</span>),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        transforms.RandomHorizontalFlip(),</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        transforms.RandomVerticalFlip(),</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        transforms.RandomRotation(</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            degrees<span class="op">=</span><span class="dv">360</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        transforms.ColorJitter(</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>            brightness<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            contrast<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>            saturation<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize(</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>            mean<span class="op">=</span>transform_mean,</span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>            std<span class="op">=</span>transform_std,</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="model-evaluation-validation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Model Evaluation &amp; Validation</h1>
<section id="validation-framework" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="validation-framework"><span class="header-section-number">4.1</span> Validation Framework</h2>
<p>To evaluate the performance of the models during training, the dataset was split into training and validation sets using a stratified split with a ratio of <em>80:20</em>. The validation set was used to monitor performance during training and prevent overfitting.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Subset</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [label <span class="cf">for</span> _, label <span class="kw">in</span> dataset.samples]</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>train_indices, val_indices <span class="op">=</span> train_test_split(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">range</span>(<span class="bu">len</span>(dataset)),</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    stratify<span class="op">=</span>labels,</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span>RANDOM_SEED</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Subset(dataset, train_indices)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Subset(dataset, val_indices)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since the dataset is imbalanced, a stratified split was used to ensure that the class distribution in the training and validation sets is similar. This prevents the model from overfitting the training set and ensures that it generalizes well to unseen data.</p>
<p>This split results in a training set of <em>3800</em> and a validation set of <em>950</em> samples (see <a href="#fig-train-vald-split" class="quarto-xref">Figure&nbsp;4</a>):</p>
<div id="fig-train-vald-split" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-train-vald-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/train_vald_split.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Training and Validation Set Sizes"><img src="resources/train_vald_split.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-vald-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Training and Validation Set Sizes
</figcaption>
</figure>
</div>
<p>Since there are no labels for the test set, the validation set serves as a proxy to evaluate the performance of the model on unseen data. Validation loss and accuracy were monitored during training to assess the convergence and generalization capabilities of the models:</p>
<div id="fig-loss-custom-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/loss.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Training and Validation Loss (custom CNN)"><img src="resources/custom_cnn/loss.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Training and Validation Loss (custom CNN)
</figcaption>
</figure>
</div>
<div id="fig-loss-pretrained-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/resnet/loss.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Training and Validation Loss (pre-trained CNN)"><img src="resources/resnet/loss.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Training and Validation Loss (pre-trained CNN)
</figcaption>
</figure>
</div>
<div id="fig-loss-pretrained-vit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-loss-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/vit/loss.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;7: Training and Validation Loss (pre-trained ViT)"><img src="resources/vit/loss.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-loss-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Training and Validation Loss (pre-trained ViT)
</figcaption>
</figure>
</div>
<p><a href="#fig-loss-custom-cnn" class="quarto-xref">Figure&nbsp;5</a>, <a href="#fig-loss-pretrained-cnn" class="quarto-xref">Figure&nbsp;6</a> and <a href="#fig-loss-pretrained-vit" class="quarto-xref">Figure&nbsp;7</a> show the training and validation loss curves for the custom CNN, pre-trained CNN and pre-trained ViT models, respectively. Each curve illustrates the learning dynamics of the model over successive epochs.</p>
<p>The custom CNN demonstrates a gradual reduction in both training and validation loss, converging steadily around epoch <em>60</em>. This indicates effective learning without significant overfitting, as the training and validation loss curves remain closely aligned. One could argue that the model could benefit from increased complexity and further training to improve performance.</p>
<p>The pre-trained CNN shows faster convergence, with training and validation loss stabilizing around epoch <em>25</em>. This faster convergence reflects the advantages of transfer learning, as the model uses pre-trained weights for feature extraction. Similarly, the pre-trained ViT, which also converges rapidly within <em>25</em> epochs, demonstrates the effectiveness of using pre-trained models for this classification task. However the gap between training and validation loss suggests that the model could benefit from additional regularization or fine-tuning to improve generalization. In particular, the pre-trained CNN begins to clearly overfit the data after epoch <em>30</em>.</p>
<p>The inclusion of a checkpoint in each figure highlights the point at which the model achieved the lowest validation loss, signaling optimal performance and serving as a reference for saving the best model state. The loaded checkpoints are at epoch <em>60</em>, <em>26</em> and <em>25</em> for the custom CNN, pre-trained CNN and pre-trained ViT, respectively.</p>
</section>
<section id="performance-metrics" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">4.2</span> Performance Metrics</h2>
<p>In addition to losses, validation accuracy is tracked during training to monitor performance of the models. Accuracy is calculated as the ratio of correctly predicted samples to the total number of samples in the validation set:</p>
<p><span id="eq-accuracy"><span class="math display">\[
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Samples}}
\tag{4}\]</span></span></p>
<p>The accuracy (<a href="#eq-accuracy" class="quarto-xref">Equation&nbsp;4</a>) provides a simple and intuitive measure of performance on the validation set.</p>
<div id="fig-accuracy-custom-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-accuracy-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/accuracy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;8: Validation Accuracy (custom CNN)"><img src="resources/custom_cnn/accuracy.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-accuracy-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Validation Accuracy (custom CNN)
</figcaption>
</figure>
</div>
<div id="fig-accuracy-pretrained-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-accuracy-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/resnet/accuracy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;9: Validation Accuracy (pre-trained CNN)"><img src="resources/resnet/accuracy.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-accuracy-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Validation Accuracy (pre-trained CNN)
</figcaption>
</figure>
</div>
<div id="fig-accuracy-pretrained-vit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-accuracy-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/vit/accuracy.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;10: Validation Accuracy (pre-trained ViT)"><img src="resources/vit/accuracy.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-accuracy-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Validation Accuracy (pre-trained ViT)
</figcaption>
</figure>
</div>
<p>Similar to the loss curves, <a href="#fig-accuracy-custom-cnn" class="quarto-xref">Figure&nbsp;8</a>, <a href="#fig-accuracy-pretrained-cnn" class="quarto-xref">Figure&nbsp;9</a> and <a href="#fig-accuracy-pretrained-vit" class="quarto-xref">Figure&nbsp;10</a> show the validation accuracy of the custom CNN, pre-trained CNN and pre-trained ViT models, respectively. The accuracy curves provide insight into the ability of the model to correctly classify the validation samples over successive epochs. The figures highlight the information from the loss curves, showing that the pre-trained models converge faster and achieve higher accuracy compared to the custom CNN. But again, the custom CNN shows a steady and smooth increase in accuracy over time, indicating that the model continues to learn and improve its performance. The selected checkpoints are the same as for the loss curves, indicating the load state of the model with the lowest validation loss, which also corresponds to high accuracy.</p>
</section>
</section>
<section id="results-analysis" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results &amp; Analysis</h1>
<section id="quantitative-results" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="quantitative-results"><span class="header-section-number">5.1</span> Quantitative Results</h2>
<p>The final results for each model are presented below:</p>
<div id="tbl-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Quantitative results of the models on the train, validation and test set.
</figcaption>
<div aria-describedby="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Train Loss</th>
<th>Val Loss</th>
<th>Val Accuracy</th>
<th>Test F1-Score</th>
<th>Epochs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Guessing Baseline</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.14105</td>
<td>-</td>
</tr>
<tr class="even">
<td>Custom CNN</td>
<td>0.2897</td>
<td>0.3034</td>
<td>0.9042</td>
<td>0.92695</td>
<td>64</td>
</tr>
<tr class="odd">
<td>Pre-trained CNN</td>
<td><strong>0.0532</strong></td>
<td><strong>0.1467</strong></td>
<td><strong>0.9537</strong></td>
<td>0.96095</td>
<td>30</td>
</tr>
<tr class="even">
<td>Pre-trained ViT</td>
<td>0.1438</td>
<td>0.2089</td>
<td>0.9379</td>
<td>0.96725</td>
<td>29</td>
</tr>
<tr class="odd">
<td><strong>Ensemble</strong></td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><strong>0.97103</strong></td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-results" class="quarto-xref">Table&nbsp;1</a> shows the performance of each model on the validation and the test set. The <em>custom CNN</em> achieved a validation accuracy of <em>90.42%</em> and a test F1-score of <em>0.92695</em>. The <em>pre-trained CNN</em> (ResNet-18) outperformed the custom CNN with a validation accuracy of <em>95.37%</em> and a test F1-score of <em>0.96095</em>. The <em>pre-trained ViT</em> achieved a validation accuracy of <em>93.79%</em> and a test F1-score of <em>0.96725</em>. The <em>ensemble</em>, which combines the predictions of all three models, achieved the highest test F1-score of <em>0.97103</em>. Since the <em>guessing baseline</em> and the <em>ensemble</em> are not trained models, the training and validation losses are not applicable.</p>
</section>
<section id="qualitative-results" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="qualitative-results"><span class="header-section-number">5.2</span> Qualitative Results</h2>
<p>Since the labels for the test set are not available, the qualitative results are based on the validation set to get an idea of the performance of the models. The confusion matrices of the custom CNN, pre-trained CNN and pre-trained ViT models on the validation set are shown below:</p>
<div id="fig-confusion-matrix-custom-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confusion-matrix-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/confusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;11: Confusion Matrix (custom CNN)"><img src="resources/custom_cnn/confusion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confusion-matrix-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Confusion Matrix (custom CNN)
</figcaption>
</figure>
</div>
<p><a href="#fig-confusion-matrix-custom-cnn" class="quarto-xref">Figure&nbsp;11</a> shows the confusion matrix of the custom CNN on the validation set. The rows represent the true classes, while the columns represent the predicted classes. The diagonal elements represent the number of correct predictions for each class, while the off-diagonal elements represent the misclassifications. While there are some misclassifications without a clear pattern, the model clearly struggles to distinguish between “Loose Silky-bent” and “Black-grass”.</p>
<div id="fig-confusion-matrix-pretrained-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confusion-matrix-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/resnet/confusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;12: Confusion Matrix (pre-trained CNN)"><img src="resources/resnet/confusion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confusion-matrix-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Confusion Matrix (pre-trained CNN)
</figcaption>
</figure>
</div>
<p>Similar to the custom CNN, <a href="#fig-confusion-matrix-pretrained-cnn" class="quarto-xref">Figure&nbsp;12</a> shows the confusion matrix of the pre-trained CNN on the validation set. The model shows the same difficulty in distinguishing between “Loose Silky-bent” and “Black-grass”.</p>
<div id="fig-confusion-matrix-pretrained-vit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confusion-matrix-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/vit/confusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;13: Confusion Matrix (pre-trained ViT)"><img src="resources/vit/confusion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confusion-matrix-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Confusion Matrix (pre-trained ViT)
</figcaption>
</figure>
</div>
<p>Although the pre-trained ViT model takes a different approach, <a href="#fig-confusion-matrix-pretrained-vit" class="quarto-xref">Figure&nbsp;13</a> shows that it also struggles with the same pair of classes “Loose Silky-bent” and “Black-grass”.</p>
</section>
<section id="comparative-analysis" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="comparative-analysis"><span class="header-section-number">5.3</span> Comparative Analysis</h2>
<p>The results show that the ensemble model outperforms the individual models, achieving the highest test F1-score of <em>0.97103</em>. The pre-trained ViT model achieved the highest individual test F1-score of <em>0.96725</em>, followed by the pre-trained CNN with a test F1-score of <em>0.96095</em>. The custom CNN achieved a test F1-score of <em>0.92695</em>, demonstrating competitive performance despite being trained from scratch.</p>
<p>Since real-time inference is not required for this task, the computational cost of the models is not a primary concern. However, the custom CNN is the lightest model with approximately <em>2 million</em> parameters, making it computationally efficient. The pre-trained CNN has approximately <em>11 million</em> parameters, while the pre-trained ViT has approximately <em>85 million</em> parameters, making it the most computationally expensive model.</p>
</section>
<section id="interpretability-measures" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="interpretability-measures"><span class="header-section-number">5.4</span> Interpretability Measures</h2>
<p>The Pytorch-Grad-CAM library <span class="citation" data-cites="jacobgilpytorchcam">(<a href="#ref-jacobgilpytorchcam" role="doc-biblioref">Gildenblat and contributors 2021</a>)</span> by Jacob Gildenblat was used to generate class activation maps (CAMs) for the custom CNN, pre-trained CNN and pre-trained ViT models. CAMs provide insight into the regions of the image that the model focuses on when making predictions and can help explain the decision-making process of the model. The library was installed with the following command:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install grad-cam</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The simplified code snippet below shows how to generate CAMs for a given image using the custom CNN model:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_grad_cam <span class="im">import</span> GradCAM</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_grad_cam.utils.image <span class="im">import</span> show_cam_on_image</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pytorch_grad_cam.utils.model_targets <span class="im">import</span> ClassifierOutputTarget</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> GradCAM(</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>      model<span class="op">=</span>model,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>      target_layers<span class="op">=</span>target_layers,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>     ) <span class="im">as</span> cam:</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    grayscale_cam <span class="op">=</span> cam(</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>                        input_tensor<span class="op">=</span>image.unsqueeze(<span class="dv">0</span>),</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>                        targets<span class="op">=</span>targets,</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    grayscale_cam <span class="op">=</span> grayscale_cam[<span class="dv">0</span>, :]</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    visualization <span class="op">=</span> show_cam_on_image(</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>                        rgb_img,</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>                        grayscale_cam,</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>                        use_rgb<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>                    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The following figures show Grad-CAM visualizations for the last two layers of the custom CNN, pre-trained CNN and pre-trained ViT models:</p>
<div id="fig-grad-cam-custom-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grad-cam-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/grad_cam.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;14: Grad-CAM (custom CNN)"><img src="resources/custom_cnn/grad_cam.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grad-cam-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Grad-CAM (custom CNN)
</figcaption>
</figure>
</div>
<div id="fig-grad-cam-pretrained-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grad-cam-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/resnet/grad_cam.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;15: Grad-CAM (pre-trained CNN)"><img src="resources/resnet/grad_cam.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grad-cam-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Grad-CAM (pre-trained CNN)
</figcaption>
</figure>
</div>
<div id="fig-grad-cam-pretrained-vit" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-grad-cam-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/vit/grad_cam.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;16: Grad-CAM (pre-trained ViT)"><img src="resources/vit/grad_cam.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-grad-cam-pretrained-vit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Grad-CAM (pre-trained ViT)
</figcaption>
</figure>
</div>
<p>The <a href="#fig-grad-cam-custom-cnn" class="quarto-xref">Figure&nbsp;14</a>, <a href="#fig-grad-cam-pretrained-cnn" class="quarto-xref">Figure&nbsp;15</a> and <a href="#fig-grad-cam-pretrained-vit" class="quarto-xref">Figure&nbsp;16</a> show the Grad-CAM visualizations for the custom CNN, pre-trained CNN and pre-trained ViT models. The visualizations highlight the regions of the image that the model focuses on when making predictions. The Grad-CAM visualizations provide insight into the decision-making process of the models and help to interpret their predictions.</p>
<p>For some classes, such as “Small-flowered Cranesbill”, “Fat Hen”, “Common Chickweed”, “Cleavers” and “Maize”, the custom CNN clearly focuses on parts of the plants that a human would use to distinguish between the classes. For these species, the focus is on the leaves. For classes like “Black-grass”, “Common wheat”, “Sugar beet”, “Scentsless Mayweed” and “Loose Silky-bent” the CNN focuses on what appears to be the soil or the background. One could argue that the model has difficulty distiguishing between the plants and the background and focuses on <em>noise</em> in the images.</p>
<p>In comparison, both pre-trained architectures, the CNN and the ViT, focus more on the plants themselves. But even for “Black grass”, “Scentsless Mayweed” and “Loose Silky-bent” the models do not seem to focus on the plants alone. The visualizations of the areas of interest are smoother for the CNN compared to the ViT, which looks more <em>blocky</em>. This is due to the different architectures and the way the models process the images as the ViT divides the image into patches and processes them separately, therefore the patches are more visible in the Grad-CAM visualizations for the ViT. For a human observer the Grad-CAM visualizations can help understand how the models make their predictions and what features they focus on, the custom CNN seems to produce the most resonable visualizations and focus on understandable features.</p>
</section>
</section>
<section id="conclusion-lessons-learned" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion &amp; Lessons Learned</h1>
<section id="key-takeaways" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">6.1</span> Key Takeaways</h2>
<p>In this project, several strategies were explored to classify plant seedlings into 12 different species, with the overall goal of achieving robust performance as measured by the mean (micro-averaged) F1-score. Data augmentation played a central role in preventing overfitting and improving model performance. Techniques such as random rotations, flips and color jittering effectively increased the diversity of the training samples, thereby improving the robustness of the learned feature representations. Meanwhile, the choice of an appropriate model architecture proved critical. A custom CNN designed and trained from scratch achieved competitive results (F1-score of 0.92695), demonstrating the potential for custom solutions even with relatively modest dataset sizes. However, the use of pre-trained networks, such as ResNet-18 and vit-base-patch16-224, demonstrated how transfer learning can deliver superior results (F1-scores of 0.96095 and 0.96725) by building on rich feature embeddings learned from large-scale datasets. Proper validation underpinned these successes, with a stratified split ensuring balanced class distributions in both the training and validation sets. This practice not only prevented the model from overfitting to majority classes, but also allowed careful monitoring of loss and accuracy metrics to guide training decisions and allowed early stopping to load the best model state before overfitting occurred.</p>
</section>
<section id="challenges-encountered" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="challenges-encountered"><span class="header-section-number">6.2</span> Challenges Encountered</h2>
<p>Despite the encouraging results, several challenges remained throughout the process. The class imbalance present in the dataset contributed to occasional misclassifications, underscoring the need for robust strategies to deal with skewed data. In addition, certain class pairs, such as “Loose Silky-bent” and “Black-grass”, exhibited high visual similarity, leading to consistent confusion for both the custom and pre-trained models. Overfitting remained a significant risk due to the limited dataset size, necessitating the use of multiple regularization methods including weight decay, dropout layers and data augmentation to ensure generalization. Computational constraints also played a role in decisions regarding batch size, image resolution and the complexity of architectures that could be feasibly trained within the available resources (e.g., freezing layers in the ViT model to reduce trainable parameters). Finally, the lack of labeled test data made it difficult to comprehensively evaluate the models, necessitating the use of the validation set as a proxy for performance on unseen data.</p>
</section>
<section id="future-work" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="future-work"><span class="header-section-number">6.3</span> Future Work</h2>
<p>Going forward, there are several ways to refine and extend the current results. Adding more models to the ensemble or training the ensemble on the validation set to find the optimal weights for each model. Exploring deeper pre-trained networks such as ResNet-50, DenseNet, or EfficientNet could improve performance, although careful management of overfitting will be important. Collecting additional labeled data or generating synthetic samples using generative adversarial networks (GANs) could help address the class imbalance and improve the ability of the model to generalize to underrepresented classes.</p>
</section>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-5206848ImageNet" class="csl-entry" role="listitem">
Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. <span>“ImageNet: A Large-Scale Hierarchical Image Database.”</span> In <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em>, 248–55. <a href="https://doi.org/10.1109/CVPR.2009.5206848">https://doi.org/10.1109/CVPR.2009.5206848</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-2010-11929" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-jacobgilpytorchcam" class="csl-entry" role="listitem">
Gildenblat, Jacob, and contributors. 2021. <span>“PyTorch Library for CAM Methods.”</span> <a href="https://github.com/jacobgil/pytorch-grad-cam" class="uri">https://github.com/jacobgil/pytorch-grad-cam</a>; GitHub.
</div>
<div id="ref-DBLP:journals/corr/abs-1711-05458" class="csl-entry" role="listitem">
Giselsson, Thomas Mosgaard, Rasmus Nyholm Jørgensen, Peter Kryger Jensen, Mads Dyrmann, and Henrik Skov Midtiby. 2017. <span>“A Public Image Database for Benchmark of Plant Seedling Classification Algorithms.”</span> <em>CoRR</em> abs/1711.05458. <a href="http://arxiv.org/abs/1711.05458">http://arxiv.org/abs/1711.05458</a>.
</div>
<div id="ref-DBLP:journals/corr/HeZRS15" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep Residual Learning for Image Recognition.”</span> <em>CoRR</em> abs/1512.03385. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-plant-seedlings-classification" class="csl-entry" role="listitem">
inversion. 2017a. <span>“Plant Seedlings Classification.”</span> <a href="https://kaggle.com/competitions/plant-seedlings-classification" class="uri">https://kaggle.com/competitions/plant-seedlings-classification</a>.
</div>
<div id="ref-plant-seedlings-classification-evaluation" class="csl-entry" role="listitem">
———. 2017b. <span>“Plant Seedlings Classification.”</span> <a href="www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation" class="uri">www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation</a>.
</div>
<div id="ref-kingma2017adammethodstochasticoptimization" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>“Adam: A Method for Stochastic Optimization.”</span> <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>.
</div>
<div id="ref-Wightman_PyTorch_Image_Models" class="csl-entry" role="listitem">
Wightman, Ross. n.d. <span>“<span>PyTorch Image Models</span>.”</span> <a href="https://doi.org/10.5281/zenodo.4414861">https://doi.org/10.5281/zenodo.4414861</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>