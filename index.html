<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-12-30">

<title>Plant Seedlings Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7bfb9ce031f0150bcdd0059ffcd3ab40.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Plant Seedlings Classification">
<meta name="citation_abstract" content="This paper explores the Plant Seedlings Classification dataset, which consists of images representing *12* plant species. Several deep learning approaches are presented, including a custom CNN trained from scratch, a pre-trained CNN (ResNet-18) and a pre-trained VisionTransformer tuned for this classification task. To mitigate the challenges of data scarcity and class imbalance, extensive data augmentation techniques such as random rotations, flips and color jittering are employed. Results show that transfer learning with ResNet-18 outperforms the custom model, achieving a mean F1 score (micro-averaged) of 0.96095 on the test set. The custom CNN, while slightly less accurate, still achieves a competitive F1 score of 0.92695, demonstrating that even smaller locally trained architectures can be viable if carefully designed and thoroughly regularized. Finally, potential solutions are outlined, including model ensembling, deeper architectures, synthetic augmentation and interpretability measures, to further improve seedling classification performance.
">
<meta name="citation_author" content="Luca Uckermann">
<meta name="citation_author" content="Nikethan Nimalakumaran">
<meta name="citation_author" content="Jonas Möller">
<meta name="citation_publication_date" content="2024-12-30">
<meta name="citation_cover_date" content="2024-12-30">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-12-30">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=A public image database for benchmark of plant seedling classification algorithms;,citation_author=Thomas Mosgaard Giselsson;,citation_author=Rasmus Nyholm Jørgensen;,citation_author=Peter Kryger Jensen;,citation_author=Mads Dyrmann;,citation_author=Henrik Skov Midtiby;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=http://arxiv.org/abs/1711.05458;,citation_volume=abs/1711.05458;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=An image is worth 16x16 words: Transformers for image recognition at scale;,citation_author=Alexey Dosovitskiy;,citation_author=Lucas Beyer;,citation_author=Alexander Kolesnikov;,citation_author=Dirk Weissenborn;,citation_author=Xiaohua Zhai;,citation_author=Thomas Unterthiner;,citation_author=Mostafa Dehghani;,citation_author=Matthias Minderer;,citation_author=Georg Heigold;,citation_author=Sylvain Gelly;,citation_author=Jakob Uszkoreit;,citation_author=Neil Houlsby;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://arxiv.org/abs/2010.11929;,citation_volume=abs/2010.11929;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Deep residual learning for image recognition;,citation_author=Kaiming He;,citation_author=Xiangyu Zhang;,citation_author=Shaoqing Ren;,citation_author=Jian Sun;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=http://arxiv.org/abs/1512.03385;,citation_volume=abs/1512.03385;,citation_journal_title=CoRR;">
<meta name="citation_reference" content="citation_title=Adam: A method for stochastic optimization;,citation_author=Diederik P. Kingma;,citation_author=Jimmy Ba;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1412.6980;">
<meta name="citation_reference" content="citation_title=Plant seedlings classification;,citation_author=undefined inversion;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_publisher=https://kaggle.com/competitions/plant-seedlings-classification;">
<meta name="citation_reference" content="citation_title=Plant seedlings classification;,citation_author=undefined inversion;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_publisher=www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation;">
<meta name="citation_reference" content="citation_title=PyTorch Image Models;,citation_author=Ross Wightman;,citation_fulltext_html_url=https://github.com/huggingface/pytorch-image-models;,citation_doi=10.5281/zenodo.4414861;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Plant Seedlings Classification</h1>
            <p class="subtitle lead">UGE: M2 SIA - DL Project Report</p>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Authors</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Luca Uckermann <a href="https://orcid.org/0009-0005-2957-6331" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        University of Applied Sciences (TH Köln)
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Nikethan Nimalakumaran </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Université Gustave Eiffel
                      </p>
                  </div>
                      <div class="quarto-title-meta-contents">
            <p class="author">Jonas Möller </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        Bielefeld University
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">December 30, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      </div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This paper explores the Plant Seedlings Classification dataset, which consists of images representing <em>12</em> plant species. Several deep learning approaches are presented, including a custom CNN trained from scratch, a pre-trained CNN (ResNet-18) and a pre-trained VisionTransformer tuned for this classification task. To mitigate the challenges of data scarcity and class imbalance, extensive data augmentation techniques such as random rotations, flips and color jittering are employed. Results show that transfer learning with ResNet-18 outperforms the custom model, achieving a mean F1 score (micro-averaged) of 0.96095 on the test set. The custom CNN, while slightly less accurate, still achieves a competitive F1 score of 0.92695, demonstrating that even smaller locally trained architectures can be viable if carefully designed and thoroughly regularized. Finally, potential solutions are outlined, including model ensembling, deeper architectures, synthetic augmentation and interpretability measures, to further improve seedling classification performance.</p>
      </div>
    </div>


    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction-problem-understanding" id="toc-introduction-problem-understanding" class="nav-link active" data-scroll-target="#introduction-problem-understanding"><span class="header-section-number">1</span> Introduction &amp; Problem Understanding</a>
  <ul class="collapse">
  <li><a href="#context-and-background" id="toc-context-and-background" class="nav-link" data-scroll-target="#context-and-background"><span class="header-section-number">1.1</span> Context and Background</a></li>
  <li><a href="#problem-definition-and-objectives" id="toc-problem-definition-and-objectives" class="nav-link" data-scroll-target="#problem-definition-and-objectives"><span class="header-section-number">1.2</span> Problem Definition and Objectives</a></li>
  <li><a href="#dataset-overview" id="toc-dataset-overview" class="nav-link" data-scroll-target="#dataset-overview"><span class="header-section-number">1.3</span> Dataset Overview</a></li>
  <li><a href="#key-challenges" id="toc-key-challenges" class="nav-link" data-scroll-target="#key-challenges"><span class="header-section-number">1.4</span> Key Challenges</a></li>
  </ul></li>
  <li><a href="#model-architecture-design" id="toc-model-architecture-design" class="nav-link" data-scroll-target="#model-architecture-design"><span class="header-section-number">2</span> Model Architecture Design</a>
  <ul class="collapse">
  <li><a href="#guessing-baseline" id="toc-guessing-baseline" class="nav-link" data-scroll-target="#guessing-baseline"><span class="header-section-number">2.1</span> Guessing Baseline</a></li>
  <li><a href="#custom-cnn" id="toc-custom-cnn" class="nav-link" data-scroll-target="#custom-cnn"><span class="header-section-number">2.2</span> Custom CNN</a></li>
  <li><a href="#pre-trained-cnn" id="toc-pre-trained-cnn" class="nav-link" data-scroll-target="#pre-trained-cnn"><span class="header-section-number">2.3</span> Pre-trained CNN</a></li>
  <li><a href="#pre-trained-vision-transformer-vit" id="toc-pre-trained-vision-transformer-vit" class="nav-link" data-scroll-target="#pre-trained-vision-transformer-vit"><span class="header-section-number">2.4</span> Pre-trained Vision Transformer (ViT)</a></li>
  <li><a href="#ensemble-model" id="toc-ensemble-model" class="nav-link" data-scroll-target="#ensemble-model"><span class="header-section-number">2.5</span> Ensemble Model</a></li>
  </ul></li>
  <li><a href="#training-optimization-strategies" id="toc-training-optimization-strategies" class="nav-link" data-scroll-target="#training-optimization-strategies"><span class="header-section-number">3</span> Training Optimization Strategies</a>
  <ul class="collapse">
  <li><a href="#training-algorithms-optimizers" id="toc-training-algorithms-optimizers" class="nav-link" data-scroll-target="#training-algorithms-optimizers"><span class="header-section-number">3.1</span> Training Algorithms &amp; Optimizers</a></li>
  <li><a href="#learning-rate-schedules" id="toc-learning-rate-schedules" class="nav-link" data-scroll-target="#learning-rate-schedules"><span class="header-section-number">3.2</span> Learning Rate Schedules</a></li>
  <li><a href="#regularization-techniques" id="toc-regularization-techniques" class="nav-link" data-scroll-target="#regularization-techniques"><span class="header-section-number">3.3</span> Regularization Techniques</a></li>
  </ul></li>
  <li><a href="#model-evaluation-validation" id="toc-model-evaluation-validation" class="nav-link" data-scroll-target="#model-evaluation-validation"><span class="header-section-number">4</span> Model Evaluation &amp; Validation</a>
  <ul class="collapse">
  <li><a href="#validation-framework" id="toc-validation-framework" class="nav-link" data-scroll-target="#validation-framework"><span class="header-section-number">4.1</span> Validation Framework</a></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="header-section-number">4.2</span> Performance Metrics</a></li>
  </ul></li>
  <li><a href="#results-analysis" id="toc-results-analysis" class="nav-link" data-scroll-target="#results-analysis"><span class="header-section-number">5</span> Results &amp; Analysis</a>
  <ul class="collapse">
  <li><a href="#quantitative-results" id="toc-quantitative-results" class="nav-link" data-scroll-target="#quantitative-results"><span class="header-section-number">5.1</span> Quantitative Results</a></li>
  <li><a href="#qualitative-results" id="toc-qualitative-results" class="nav-link" data-scroll-target="#qualitative-results"><span class="header-section-number">5.2</span> Qualitative Results</a></li>
  <li><a href="#comparative-analysis" id="toc-comparative-analysis" class="nav-link" data-scroll-target="#comparative-analysis"><span class="header-section-number">5.3</span> Comparative Analysis</a></li>
  </ul></li>
  <li><a href="#conclusion-lessons-learned" id="toc-conclusion-lessons-learned" class="nav-link" data-scroll-target="#conclusion-lessons-learned"><span class="header-section-number">6</span> Conclusion &amp; Lessons Learned</a>
  <ul class="collapse">
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways"><span class="header-section-number">6.1</span> Key Takeaways</a></li>
  <li><a href="#challenges-encountered" id="toc-challenges-encountered" class="nav-link" data-scroll-target="#challenges-encountered"><span class="header-section-number">6.2</span> Challenges Encountered</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work"><span class="header-section-number">6.3</span> Future Work</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">7</span> References</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction-problem-understanding" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Introduction &amp; Problem Understanding</h1>
<section id="context-and-background" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="context-and-background"><span class="header-section-number">1.1</span> Context and Background</h2>
<p>The “Plant Seedlings Classification” challenge, hosted on Kaggle <span class="citation" data-cites="plant-seedlings-classification">(<a href="#ref-plant-seedlings-classification" role="doc-biblioref">inversion 2017a</a>)</span>, presents a real-world problem central to modern agriculture: accurately identifying the species of young seedlings from digital images.</p>
<p>The dataset described in the paper “A Public Image Database for Benchmark of Plant Seedling Classification Algorithms” <span class="citation" data-cites="DBLP:journals/corr/abs-1711-05458">(<a href="#ref-DBLP:journals/corr/abs-1711-05458" role="doc-biblioref">Giselsson et al. 2017</a>)</span> contains images of approximately <em>960</em> unique plants, representing <em>12</em> different species. Each image captures a seedling at different growth stages and under different conditions, reflecting the complexities found in real-world agricultural environments. These conditions include differences in lighting, background soil patterns and subtle phenotypic variations that can blur the lines between certain species. The evaluation metric of the competition is a mean F-score (micro-averaged F1-score), which encourages balanced performance across classes <span class="citation" data-cites="plant-seedlings-classification-evaluation">(<a href="#ref-plant-seedlings-classification-evaluation" role="doc-biblioref">inversion 2017b</a>)</span>:</p>
<p><span id="eq-precision"><span class="math display">\[
\text{Precision}_{\text{micro}} = \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FP_k}
\tag{1}\]</span></span></p>
<p><span id="eq-recall"><span class="math display">\[
\text{Recall}_{\text{micro}} = \frac{\sum_{k \in C} TP_k}{\sum_{k \in C} TP_k + FN_k}
\tag{2}\]</span></span></p>
<p><span id="eq-fscore"><span class="math display">\[
F1_{\text{micro}} = \frac{2 \cdot \text{Precision}_{\text{micro}} \cdot \text{Recall}_{\text{micro}}}{\text{Precision}_{\text{micro}} + \text{Recall}_{\text{micro}}}
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(TP_i\)</span>, <span class="math inline">\(FP_i\)</span> and <span class="math inline">\(FN_i\)</span> are the true positive, false positive and false negative counts for class <span class="math inline">\(i\)</span>, respectively and <span class="math inline">\(C\)</span> is the set of all classes. The mean F-score (<a href="#eq-fscore" class="quarto-xref">Equation&nbsp;3</a>) is a balanced measure that considers both precision (<a href="#eq-precision" class="quarto-xref">Equation&nbsp;1</a>) and recall (<a href="#eq-recall" class="quarto-xref">Equation&nbsp;2</a>) across all classes, making it a suitable evaluation metric for multi-class classification tasks.</p>
</section>
<section id="problem-definition-and-objectives" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="problem-definition-and-objectives"><span class="header-section-number">1.2</span> Problem Definition and Objectives</h2>
<p>The core objective of the challenge is to build an automated classification model that can take a seedling image as input and accurately predict its species. The following points summarize the task:</p>
<ul>
<li><strong>Input:</strong> Set of 794 images of plant seedlings.</li>
<li><strong>Output:</strong> Classification label for each image, indicating the species of the plant seedling.</li>
<li><strong>Goal:</strong> High classification performance as measured by the <a href="#eq-fscore" class="quarto-xref">Equation&nbsp;3</a>.</li>
</ul>
</section>
<section id="dataset-overview" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="dataset-overview"><span class="header-section-number">1.3</span> Dataset Overview</h2>
<p>For a better understanding of the dataset, a brief overview of the class distribution and sample images is provided below:</p>
<div id="fig-class-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-class-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/class_distribution.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Class Distribution"><img src="resources/class_distribution.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-class-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Class Distribution
</figcaption>
</figure>
</div>
<p><a href="#fig-class-distribution" class="quarto-xref">Figure&nbsp;1</a> shows the distribution of classes in the dataset, with each bar representing the number of images per class. The dataset is imbalanced, with some classes having significantly fewer samples than others. This imbalance can pose a challenge for model training, as the model may struggle to learn the features of underrepresented classes effectively. The most common classes are “Loose Silky-bent” and “Common Chickweed”, while the least common classes are “Common wheat” and “Maize”.</p>
<div id="fig-sample-images" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sample-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/sample_images.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Sample Images"><img src="resources/sample_images.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sample-images-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Sample Images
</figcaption>
</figure>
</div>
<p><a href="#fig-sample-images" class="quarto-xref">Figure&nbsp;2</a> shows a sample image for each class in the dataset, illustrating the visual diversity in different species. The images vary in background, lighting and growth stage, highlighting the challenges of visual similarity across species.</p>
</section>
<section id="key-challenges" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="key-challenges"><span class="header-section-number">1.4</span> Key Challenges</h2>
<p>Developing robust classification models for this task is not trivial. There are several challenges:</p>
<ol type="1">
<li><strong>Visual Similarity Among Species:</strong> Certain seedlings can look strikingly similar, making it difficult for both humans and machines to distinguish between them.</li>
<li><strong>Intra-Class Variability:</strong> Even within a single species, seedlings can vary significantly in appearance due to differences in growth stage, lighting and background. This variability challenges models to learn consistent features that generalize well.</li>
<li><strong>Data Limitations:</strong> With approximately 960 unique plants, the dataset could be considered modest for training deep learning models from scratch. While data augmentation can help to some extent, the relatively small dataset may still limit the complexity of models that can be effectively trained without overfitting.</li>
<li><strong>Model Architecture Complexity:</strong> Choosing the right model architecture, whether a custom CNN trained from scratch or a pre-trained deep CNN, to learn complex visual features. Deeper models can capture more nuanced differences, but they can also be harder to train and require careful regularization to prevent overfitting.</li>
</ol>
<p>By clearly understanding these challenges and the broader context, model architectures can be proposed that address these difficulties. The following chapters discuss the strategies for model design, training optimization and thorough evaluation, ultimately leading to the approach that best addresses the core challenge of differentiating between plant seedling species.</p>
</section>
</section>
<section id="model-architecture-design" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Model Architecture Design</h1>
<p>To ensure deterministic results and reproducibility, the random seed <em>42</em> is set at the beginning of each script:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>RANDOM_SEED <span class="op">=</span> <span class="dv">42</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>seed(RANDOM_SEED)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>np.random.seed(RANDOM_SEED)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(RANDOM_SEED)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed_all(RANDOM_SEED)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.deterministic <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.benchmark <span class="op">=</span> <span class="va">False</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="guessing-baseline" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="guessing-baseline"><span class="header-section-number">2.1</span> Guessing Baseline</h2>
<p>As a starting point and to get familiar with the dataset and the Kaggle competition, a simple guessing baseline is implemented. The baseline assigns the most frequent class label to all test samples. This approach provides a lower bound on model performance and serves as a reference point for evaluating the effectiveness of more sophisticated models. The head of the submission file (<code>submission-0.14105_Loose-Silky-bent.csv</code>) is shown below:</p>
<pre class="csv"><code>file,species
1b490196c.png,Loose Silky-bent
85431c075.png,Loose Silky-bent
506347cfe.png,Loose Silky-bent
7f46a71db.png,Loose Silky-bent
668c1007c.png,Loose Silky-bent
...</code></pre>
</section>
<section id="custom-cnn" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="custom-cnn"><span class="header-section-number">2.2</span> Custom CNN</h2>
<p>The custom CNN architecture is designed to capture features relevant to seedling classification, while being lightweight enough to be effectively trained locally on the given dataset. The model consists of a series of convolutional and pooling layers followed by fully connected layers to learn hierarchical features and make class predictions.</p>
<div id="fig-custom-cnn-architecture" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-custom-cnn-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/architecture.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;3: Custom CNN Architecture"><img src="resources/custom_cnn/architecture.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-custom-cnn-architecture-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Custom CNN Architecture
</figcaption>
</figure>
</div>
<p><a href="#fig-custom-cnn-architecture" class="quarto-xref">Figure&nbsp;3</a> shows the described architecture.</p>
</section>
<section id="pre-trained-cnn" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="pre-trained-cnn"><span class="header-section-number">2.3</span> Pre-trained CNN</h2>
<p>As an alternative to training a custom CNN from scratch, a pre-trained CNN can be used to leverage learned features from a large dataset. The pre-trained model ResNet-18 <span class="citation" data-cites="DBLP:journals/corr/HeZRS15">(<a href="#ref-DBLP:journals/corr/HeZRS15" role="doc-biblioref">He et al. 2015</a>)</span> is used as a feature extractor, where the final classification layer is replaced with a new fully connected layer to predict the 12 plant seedling classes:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Linear</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> models.resnet18(</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    weights<span class="op">=</span>models.ResNet18_Weights.DEFAULT</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>model.fc <span class="op">=</span> Linear(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    in_features<span class="op">=</span>model.fc.in_features,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    out_features<span class="op">=</span><span class="bu">len</span>(dataset.classes),</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="pre-trained-vision-transformer-vit" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="pre-trained-vision-transformer-vit"><span class="header-section-number">2.4</span> Pre-trained Vision Transformer (ViT)</h2>
<p>Another approach is to use a ViT <span class="citation" data-cites="DBLP:journals/corr/abs-2010-11929">(<a href="#ref-DBLP:journals/corr/abs-2010-11929" role="doc-biblioref">Dosovitskiy et al. 2020</a>)</span> as the backbone architecture. The ViT model is pre-trained on a large-scale dataset and then fine-tuned on the plant seedlings dataset. The final classification head is replaced with a new linear layer to predict the 12 plant seedling classes:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> timm</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> timm.create_model(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"vit_base_patch16_224"</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    pretrained<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    num_classes<span class="op">=</span>num_classes</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>model.head <span class="op">=</span> torch.nn.Linear(</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    model.head.in_features,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    num_classes</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Instead of fine-tuning the entire model, the pre-trained weights of the <code>vit_base_patch16_224</code> model <span class="citation" data-cites="Wightman_PyTorch_Image_Models">(<a href="#ref-Wightman_PyTorch_Image_Models" role="doc-biblioref">Wightman, n.d.</a>)</span> are frozen, and only the classification head is trained on the plant seedlings dataset. This approach leverages the powerful feature extraction capabilities of the pre-trained model while adapting the final layer to the specific classification task. Furthermore the computational cost is reduced compared to training the entire model from scratch.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.parameters():</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> param <span class="kw">in</span> model.head.parameters():</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    param.requires_grad <span class="op">=</span> <span class="va">True</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="ensemble-model" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="ensemble-model"><span class="header-section-number">2.5</span> Ensemble Model</h2>
<p>A final ensemble model is created by combining the predictions of all three models (custom CNN, pre-trained CNN, pre-trained ViT) using a simple weighted average. The weights are determined based on the performance of each model on the test set:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>model_custom_cnn.<span class="bu">eval</span>()</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>model_resnet.<span class="bu">eval</span>()</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>model_vit.<span class="bu">eval</span>()</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>w_custom_cnn <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>w_resnet <span class="op">=</span> <span class="fl">0.25</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>w_vit <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> w_custom_cnn <span class="op">-</span> w_resnet</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, image_names <span class="kw">in</span> test_loader:</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        probs_custom_cnn <span class="op">=</span> F.softmax(model_custom_cnn(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        probs_resnet <span class="op">=</span> F.softmax(model_resnet(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        probs_vit <span class="op">=</span> F.softmax(model_vit(images), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        probs_ensemble <span class="op">=</span> w_custom_cnn <span class="op">*</span> probs_custom_cnn <span class="op">+</span> w_resnet <span class="op">*</span> probs_resnet <span class="op">+</span> w_vit <span class="op">*</span> probs_vit</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        _, preds <span class="op">=</span> torch.<span class="bu">max</span>(probs_ensemble, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The ensemble model combines the strengths of each individual model to improve overall performance and robustness. By averaging the predictions of multiple models, the ensemble model can reduce the impact of individual model weaknesses and provide more reliable predictions. As the ViT model achieved the highest performance on the test set, it is assigned the highest weight in the ensemble (0.5), while the custom CNN and ResNet models are assigned equal weights (0.25).</p>
</section>
</section>
<section id="training-optimization-strategies" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Training Optimization Strategies</h1>
<section id="training-algorithms-optimizers" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="training-algorithms-optimizers"><span class="header-section-number">3.1</span> Training Algorithms &amp; Optimizers</h2>
<p>All models were trained using the Adam optimizer <span class="citation" data-cites="kingma2017adammethodstochasticoptimization">(<a href="#ref-kingma2017adammethodstochasticoptimization" role="doc-biblioref">Kingma and Ba 2017</a>)</span> with a learning rate of <em>0.001</em>. The Adam optimizer is a popular choice for training deep neural networks due to its adaptive learning rate mechanism and momentum-based updates. A weight decay of <em>1e-4</em> was applied to regularize the model and prevent overfitting.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> Adam(model.parameters(),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    lr<span class="op">=</span><span class="fl">1e-3</span>,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    weight_decay<span class="op">=</span><span class="fl">1e-4</span>,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="learning-rate-schedules" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="learning-rate-schedules"><span class="header-section-number">3.2</span> Learning Rate Schedules</h2>
<p>To adjust the learning rate during training, a learning rate scheduler was used to reduce the learning rate by a factor of <em>0.5</em> if the validation loss did not improve for <em>2</em> epochs. This technique helps the model converge more effectively by gradually reducing the learning rate as it approaches a local minimum.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim.lr_scheduler <span class="im">import</span> ReduceLROnPlateau</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> ReduceLROnPlateau(</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    mode<span class="op">=</span><span class="st">"min"</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    factor<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    patience<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="regularization-techniques" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="regularization-techniques"><span class="header-section-number">3.3</span> Regularization Techniques</h2>
<p>To prevent overfitting and improve generalization, several regularization techniques were applied during training:</p>
<ul>
<li><strong>Weight Decay:</strong> L2 regularization with a weight decay of <em>1e-4</em> was applied to the optimizer to penalize large weights and prevent overfitting.</li>
<li><strong>Dropout:</strong> A dropout layer with a dropout probability of <em>0.5</em> was added after the fully connected layer to regularize the model and prevent co-adaptation of neurons.</li>
<li><strong>Data Augmentation:</strong> Various data augmentation techniques such as random rotations, flips and color jittering were applied to the training images to increase the diversity of the training set and improve the robustness of the model.</li>
</ul>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> Dropout</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="va">self</span>.droupout <span class="op">=</span> Dropout(p<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose(</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        transforms.Resize(</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>transform_resize</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        transforms.RandomResizedCrop(</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>            size<span class="op">=</span>transform_resize,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>            scale<span class="op">=</span>(<span class="fl">0.8</span>, <span class="fl">1.0</span>),</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>            ratio<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">1.1</span>),</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        transforms.RandomHorizontalFlip(),</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>        transforms.RandomVerticalFlip(),</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        transforms.RandomRotation(</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            degrees<span class="op">=</span><span class="dv">360</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        transforms.ColorJitter(</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>            brightness<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>            contrast<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>            saturation<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>            hue<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>        transforms.ToTensor(),</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        transforms.Normalize(</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>            mean<span class="op">=</span>transform_mean,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            std<span class="op">=</span>transform_std,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="model-evaluation-validation" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Model Evaluation &amp; Validation</h1>
<section id="validation-framework" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="validation-framework"><span class="header-section-number">4.1</span> Validation Framework</h2>
<p>To evaluate the performance of the models during training, the dataset was split into training and validation sets using a stratified split with a ratio of <em>80:20</em>. The validation set was used to monitor performance during training and prevent overfitting.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Subset</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [label <span class="cf">for</span> _, label <span class="kw">in</span> dataset.samples]</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>train_indices, val_indices <span class="op">=</span> train_test_split(</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">range</span>(<span class="bu">len</span>(dataset)),</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    stratify<span class="op">=</span>labels,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span>RANDOM_SEED</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> Subset(dataset, train_indices)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>val_dataset <span class="op">=</span> Subset(dataset, val_indices)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Since the dataset is imbalanced, a stratified split was used to ensure that the class distribution in the training and validation sets is similar. This prevents the model from overfitting the training set and ensures that it generalizes well to unseen data.</p>
<p>This split results in a training set of <em>3800</em> and a validation set of <em>950</em> samples (see <a href="#fig-train-vald-split" class="quarto-xref">Figure&nbsp;4</a>):</p>
<div id="fig-train-vald-split" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-train-vald-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/train_vald_split.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;4: Training and Validation Set Sizes"><img src="resources/train_vald_split.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-train-vald-split-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Training and Validation Set Sizes
</figcaption>
</figure>
</div>
<p>Since there are no labels for the test set, the validation set serves as a proxy to evaluate the performance of the model on unseen data. Validation loss and accuracy were monitored during training to assess the model’s convergence and generalization capabilities.</p>
</section>
<section id="performance-metrics" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">4.2</span> Performance Metrics</h2>
<p>In addition to losses, validation accuracy is tracked during training to monitor performance of the models. Accuracy is calculated as the ratio of correctly predicted samples to the total number of samples in the validation set:</p>
<p><span id="eq-accuracy"><span class="math display">\[
\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Samples}}
\tag{4}\]</span></span></p>
<p>The accuracy (<a href="#eq-accuracy" class="quarto-xref">Equation&nbsp;4</a>) provides a simple and intuitive measure of performance on the validation set.</p>
</section>
</section>
<section id="results-analysis" class="level1" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Results &amp; Analysis</h1>
<section id="quantitative-results" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="quantitative-results"><span class="header-section-number">5.1</span> Quantitative Results</h2>
<p>The final results for each model are presented below:</p>
<div id="tbl-results" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Results of the different models on the validation and test set. The best performance for each metric is highlighted in bold.
</figcaption>
<div aria-describedby="tbl-results-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 15%">
<col style="width: 16%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Train Loss</th>
<th>Val Loss</th>
<th>Val Accuracy (%)</th>
<th>Test F1-Score</th>
<th>Epochs</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Guessing Baseline</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>0.14105</td>
<td>-</td>
</tr>
<tr class="even">
<td>Custom CNN</td>
<td>0.2897</td>
<td>0.3034</td>
<td>90.42</td>
<td>0.92695</td>
<td>64</td>
</tr>
<tr class="odd">
<td>Pre-trained CNN</td>
<td><strong>0.0532</strong></td>
<td><strong>0.1467</strong></td>
<td><strong>95.37</strong></td>
<td>0.96095</td>
<td>30</td>
</tr>
<tr class="even">
<td>Pre-trained ViT</td>
<td>0.1438</td>
<td>0.2089</td>
<td>93.79</td>
<td>0.96725</td>
<td>29</td>
</tr>
<tr class="odd">
<td>Ensemble Model</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td><strong>0.97103</strong></td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p><a href="#tbl-results" class="quarto-xref">Table&nbsp;1</a> shows the performance of each model on the validation and the test set. The <em>custom CNN</em> achieved a validation accuracy of <em>90.42%</em> and a test F1-score of <em>0.92695</em>. The <em>pre-trained CNN</em> (ResNet-18) outperformed the custom CNN with a validation accuracy of <em>95.37%</em> and a test F1-score of <em>0.96095</em>. The <em>pre-trained ViT</em> achieved a validation accuracy of <em>93.79%</em> and a test F1-score of <em>0.96725</em>. The ensemble model, which combines the predictions of all three models, achieved the highest test F1-score of <em>0.97103</em>.</p>
</section>
<section id="qualitative-results" class="level2" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="qualitative-results"><span class="header-section-number">5.2</span> Qualitative Results</h2>
<p>Since the labels for the test set are not available, the qualitative results are based on the validation set.</p>
<div id="fig-confusion-matrix-custom-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confusion-matrix-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/custom_cnn/confusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;5: Confusion Matrix (Custom CNN)"><img src="resources/custom_cnn/confusion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confusion-matrix-custom-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Confusion Matrix (Custom CNN)
</figcaption>
</figure>
</div>
<p><a href="#fig-confusion-matrix-custom-cnn" class="quarto-xref">Figure&nbsp;5</a> shows the confusion matrix of the custom CNN on the validation set. The rows represent the true classes, while the columns represent the predicted classes. The diagonal elements represent the number of correct predictions for each class, while the off-diagonal elements represent the misclassifications. While there are some misclassifications without a clear pattern, the model clearly struggles to distinguish between “Loose Silky-bent” and “Black-grass”.</p>
<div id="fig-confusion-matrix-pretrained-cnn" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-confusion-matrix-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="resources/resnet/confusion.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;6: Confusion Matrix (Pre-trained CNN)"><img src="resources/resnet/confusion.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-confusion-matrix-pretrained-cnn-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Confusion Matrix (Pre-trained CNN)
</figcaption>
</figure>
</div>
<p>Similar to the custom CNN, <a href="#fig-confusion-matrix-pretrained-cnn" class="quarto-xref">Figure&nbsp;6</a> shows the confusion matrix of the pre-trained CNN on the validation set. The model shows the same difficulty in distinguishing between “Loose Silky-bent” and “Black-grass”.</p>
</section>
<section id="comparative-analysis" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="comparative-analysis"><span class="header-section-number">5.3</span> Comparative Analysis</h2>
</section>
</section>
<section id="conclusion-lessons-learned" class="level1" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Conclusion &amp; Lessons Learned</h1>
<section id="key-takeaways" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="key-takeaways"><span class="header-section-number">6.1</span> Key Takeaways</h2>
<p>In this project, several strategies were explored to classify plant seedlings into 12 different species, with the overall goal of achieving robust performance as measured by the mean F1-score (micro-averaged). Data augmentation played a central role in mitigating overfitting and improving model performance. Techniques such as random rotations, flips and color jittering effectively increased the diversity of training samples, thereby improving the robustness of the learned feature representations. Meanwhile, the choice of an appropriate model architecture proved critical. A custom CNN designed and trained from scratch achieved competitive results (F1-score of 0.92695), demonstrating the potential for custom solutions even with relatively modest dataset sizes. However, the use of a pre-trained network, specifically ResNet-18, demonstrated how transfer learning can deliver superior results (F1-score of 0.96095) by building on rich feature embeddings learned from large-scale datasets. Proper validation underpinned these successes, with a stratified split ensuring balanced class distributions in both the training and validation sets. This practice not only prevented the model from overfitting to majority classes, but also enabled the careful monitoring of loss and accuracy metrics to guide training decisions.</p>
</section>
<section id="challenges-encountered" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="challenges-encountered"><span class="header-section-number">6.2</span> Challenges Encountered</h2>
<p>Despite the encouraging results, several challenges remained throughout the process. The class imbalance present in the dataset contributed to occasional misclassifications, underscoring the need for robust strategies to deal with skewed data. In addition, certain class pairs, such as “Loose Silky-bent” and “Black-grass”, exhibited high visual similarity, leading to consistent confusion for both the custom and pre-trained CNNs. Overfitting remained a significant risk due to the limited dataset size, necessitating the use of multiple regularization methods including weight decay, dropout layers and data augmentation to ensure generalization. Computational constraints also played a role in decisions regarding batch size, image resolution and the complexity of architectures that could feasibly be trained within the available resources.</p>
</section>
<section id="future-work" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="future-work"><span class="header-section-number">6.3</span> Future Work</h2>
<p>Going forward, there are several opportunities to refine and extend the current results. Model ensembling, where predictions from different architectures or random initialization seeds are combined, could further improve classification accuracy by exploiting a variety of learned representations. Exploring deeper pre-trained networks such as ResNet-50, DenseNet, or EfficientNet could push the performance envelope, although careful management of overfitting will be important. Collecting additional labeled data, perhaps through targeted field sampling, would alleviate some of the constraints of class imbalance, while generative methods-such as GANs-could produce synthetic examples to strengthen underrepresented classes.</p>
</section>
</section>
<section id="references" class="level1" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-DBLP:journals/corr/abs-2010-11929" class="csl-entry" role="listitem">
Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. <span>“An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.”</span> <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.
</div>
<div id="ref-DBLP:journals/corr/abs-1711-05458" class="csl-entry" role="listitem">
Giselsson, Thomas Mosgaard, Rasmus Nyholm Jørgensen, Peter Kryger Jensen, Mads Dyrmann, and Henrik Skov Midtiby. 2017. <span>“A Public Image Database for Benchmark of Plant Seedling Classification Algorithms.”</span> <em>CoRR</em> abs/1711.05458. <a href="http://arxiv.org/abs/1711.05458">http://arxiv.org/abs/1711.05458</a>.
</div>
<div id="ref-DBLP:journals/corr/HeZRS15" class="csl-entry" role="listitem">
He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. <span>“Deep Residual Learning for Image Recognition.”</span> <em>CoRR</em> abs/1512.03385. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.
</div>
<div id="ref-plant-seedlings-classification" class="csl-entry" role="listitem">
inversion. 2017a. <span>“Plant Seedlings Classification.”</span> <a href="https://kaggle.com/competitions/plant-seedlings-classification" class="uri">https://kaggle.com/competitions/plant-seedlings-classification</a>.
</div>
<div id="ref-plant-seedlings-classification-evaluation" class="csl-entry" role="listitem">
———. 2017b. <span>“Plant Seedlings Classification.”</span> <a href="www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation" class="uri">www.kaggle.com/competitions/plant-seedlings-classification/overview/evaluation</a>.
</div>
<div id="ref-kingma2017adammethodstochasticoptimization" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2017. <span>“Adam: A Method for Stochastic Optimization.”</span> <a href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</a>.
</div>
<div id="ref-Wightman_PyTorch_Image_Models" class="csl-entry" role="listitem">
Wightman, Ross. n.d. <span>“<span>PyTorch Image Models</span>.”</span> <a href="https://doi.org/10.5281/zenodo.4414861">https://doi.org/10.5281/zenodo.4414861</a>.
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>